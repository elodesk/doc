\chapter{Survey of Message Broker implementations} 
\label{survey-broker}
In previous chapters we first discussed the traditional messaging broker as it
hosts a queue for simply move data between distributed clients without losing
messages or requiring each component to be always available. Regarding to our
second topic, the streaming of big data we saw that a stream is actually an
abstract version of a message broker with special abilities optimized for
further processing events in real-time. 

In the following survey we want to compare implementations of the most related
broker system by mainly compare the following characteristics---which we already
defined in \todo{ref}---to our reference product Apache Kafka. \todo{ref}

\begin{itemize}
\item Consumption Model 
     \item Delivery Reliability 
         \item Fault Tolerance
\end{itemize}

\section{Implementations}
%\subsection{Traditional Message Broker}

\begin{description}
    \item [Rabbit MQ] \hfill \\
    {
    RabbitMQ is written in Erlang and its source code is released under the
    Mozilla Public License. RabbitMQ supports many messaging protocols, among
    which the most important are STOMP: Streaming Text Oriented messaging
    Protocol and AMQP: Advanced Messaging Queuing Protocol. Incoming messages
    are placed in a queue whereas this message can further be stored in memory
    or on a disk. The latter model of persistence will underperform when you
    have a large number of queues which need to access the disk simultaneously.
    \cite{rabbitmq}

    In a RabbitMQ Cluster, queues are created and live in a single node, and all
    nodes know about all the queues. When a node receives a request to a queue
    that is not available in the current node, it routes the request to the node
    that has the queue. To provide high availability (HA), RabbitMQ replicates
    messages between master and a mirrored slave, so the slave can take over if the
    master has died. \cite{wickramarachchi2012andes}

    What RabbitMQ clustering doesn't do is provide guarantees against message loss.
    When a Rabbit cluster node dies, the messages in queues on that
    node can disappear. This is because RabbitMQ doesn't replicate the contents
    of queues throughout the cluster. They live only on the node that owns the
    queue. \cite{videla2012rabbitmq}
    }
    \item [Active MQ] \hfill \\
    {
    Note: Apache ActiveMQ is an open source message broker written in Java
    together with a full Java Message Service (JMS) client. 
    
    The Replicated
    LevelDB Store uses Apache ZooKeeper to pick a master from a set of broker
    nodes configured to replicate a LevelDB Store. Then synchronizes all slave
    LevelDB Stores with the master keeps them up to date by replicating all
    updates from the master. 
    
    ActiveMQ will preserve the order of messages sent
    by a single producer to all consumers on a topic. If there is a single
    consumer on a queue then the order of messages sent by a single producer
    will be preserved as well. 
    
    Journal: To achieve high performance of durable
    messaging in ACtiveMQ V4.x we strongly recommend you use our high
    performance journal - which is enabled by default. This works rather like a
    database; messages (and transcation commits/rollbacks and message
    acknowledgements) are written to the journal as fast as is humanly possible
    - then at intervals we checkpoint the journal to the long term persistence
    storage (in this case JDBC).Kind of. 
    A message can be loaded directly from the journal if it was swapped out of memory.
    
    The journal cannot be used, however, to recover a durable subscription as it
    does not keep an ordered index of messages per durable sub. So when a durable
    sub is activated, the journal checkpoints to flush any messages in the journal
    to the long term store and then the long term store is used to recover the
    durable subscription.Brokers cannot share a journal. Each must be
configured with it's own journal. Broker Clustering: The most common mental model of clustering in
a JMS context is that there is a collection of JMS brokers and a JMS client
will connect to one of them; then if the JMS broker goes down, it will
f we just run multiple brokers on a network and tell the clients about them
using either static discovery or dynamic discovery, then clients can easily
failover from one broker to another. However, stand alone brokers don't know
about consumers on other brokers; so if there are no consumers on a certain
broker, messages could just pile up without being consumedauto-reconnect to
another broker. } 

\end{description}


%\subsection{Streaming Broker}
\begin{description}
    \item [Amazon SQS] \hfill \\
    {
        Amazon Simple Queuing Service is a cloud service which offers a simple messaging queue for
        move data between distributed systems. SQS guarantees reliability
        by redeliver messages in case of failure. If a message successfully
        processed by a consumer it will be deleted out of the queue.  Amazon SQS
        does not guarantee a strict ordering of messages. All messages are
        stored redundantly on multiple servers and in multiple data centers,
        which means that no single computer or network failure
        renders SQS messages inaccessible.\cite{amazonSQS} \cite{amazonSQSFaq} 

        SQS as itself can not provide a publish subscribe behaviour as we
        have in typical message brokers. But in combination with the Amazon
        Simple  Notification Service (SNS) it
        is possible to create topics which are linked to SQS queues where
        consumers can register itself and only receive message from a specific
        topic. \cite{amazonSqsPubSub}
     }
    \item [Amazon Kinesis] \hfill \\
    { 
    Amazon Kinesis is a cloud service for streaming event
    data for further processing which is very similar to Apache Kafka. Producer
    applications push data continuously to the Kinesis broker where the messages
    are keept for 24 hours in memory. Kinesis provides ordering of records, as
    well as the ability to read and/or replay records in the same order to
    multiple Amazon Kinesis Applications.
    \cite{amazonKinesis} \cite{amazonKinesisFAQ} 
    }

   \begin{figure}[H]
     \centering
     \includegraphics[width=0.7\textwidth]{images/amazon-kinesis.png}
         \caption{Amazon Kinesis aggregates events and interacts with other AWS
         Cloud services \cite{amazonKinesis}}
        \label{fig:amazon-kinesi}
    \end{figure}
    }
    \item [Scribe] \hfill \\
    { Push based, no pub-sub, static configuration}
    \item [Kastrell] \hfill \\
    {no clustering; no pub-sub}
    \item [Apache Flume] \hfill \\
    {Flume is a distributed service specialized on being a reliable way of
    getting event data into HDFS \todo{gls}. The typical deployment consists
    of a number of logical nodes, arranged into three tiers. The first tier
    is the agent tier. Agent nodes are typically installed on the machines
    that generate the data and are the initial point of contact with Flume.
    They forward data to the next tier of collector nodes, which aggregate the
    separate data flows and forward them to the final storage tier. Flume is
    push based and does not support public-subscribe semantics. \cite{apacheflumeDoc}
    \begin{figure}[H]
     \centering
     \includegraphics[width=0.4\textwidth]{images/flume-architecture.png}
         \caption{Apache Flume general architecture \cite{apacheflumeDoc}}
        \label{fig:flume-architecture}
    \end{figure}

    }
\end{description}

\begin{landscape}
\section{Conslusion}

\todo[inline]{Welches System passt für welchen Anwendungsfall und welche nicht
(Gründe)}
\end{landscape}

