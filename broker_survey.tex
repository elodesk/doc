\chapter{Survey of Message Broker implementations} 
\label{survey-broker}
In previous chapters we first discussed the traditional messaging broker as it hosts a queue
for simply move data between distributed clients without losing messages or
requiring each component to be always available. Regarding to our second topic,
the streaming of big data we defined an advanced broker with special abilities
optimized for further processing events in real-time. 

In the following survey we want to compare implementations of the most related
broker system by first defining relevant characteristics and compare the
features of each product after.

\section{Relevant Characteristics}
\begin{description}
    \item [Architecture] \hfill \\
    { }
    \item [Throughput] \hfill \\
        { (What it is) (Key Aspects)}
    \item [Scalability] \hfill \\
    {}
    \item [Latency]\hfill \\
    {elapsed time it takes to process a single message   }
    \item [Scalability] \hfill \\
    {}
    \item [Reliability / Fault Tolerance] \hfill \\
        {Note: The problem with running lots of stand alone brokers or brokers in a
        network is that messages are owned by a single physical broker at any
    point in time. If that broker goes down, you have to wait for it to be
restarted before the message can be delivered. (If you are using non-persistent
messaging and a broker goes down you generally lose your message). The idea
behind MasterSlave is that messages are replicated to a slave broker so that
even if you have a catastrophic hardware failure of the master's machine, file
system or data centre, you get immediate failover to the slave with no message
loss.}

    \item [Message delivery] \hfill \\
    {}
\item [Independence] \hfill \\
    {}
\item [Simplicity] \hfill \\
    {}
    \item [Persistency] \hfill \\ 
        Note: The main difference is that if you are using
        persistent delivery, messages are persisted to disk/database so that
        they will survive a broker restart. When using non-persistent delivery,
        if you kill a broker then you will lose all in-transit messages. \\
        Queues: \\
        Persistent : Message will be saved on disk and sent later if consumer is inactive.\\
        Non-persistent : Messages will be saved in-memory and sent later if
        consumer is inactive. But they will not survive a broker re-start. \\
        Topics: 
        Persistent and Durable : Message will be saved both on-disk and in-memory
        and sent later if subscriber is inactive.\\
        Persistent  and  Non-Durable :
        Message will not be saved either in-memory or on disk and any inactive
        subscriber at the moment of message receipt at broker will not receive
        the message.\\
        Non-persistent and Durable : Message will be saved
        in-memory and sent later if subscriber is inactive. So it will not
        survive a broker re-start.\\
        Non-persistent and Non-Durable : Message
        will not be saved either in-memory or on disk and any inactive
        subscriber at the moment of message receipt at broker will not receive
        the message. Similar to Persistent and Non-Durable case.\\


\end{description}
\section{Implementations}
%\subsection{Traditional Message Broker}

\begin{description}
    \item [Active MQ] \hfill \\
        {Note: Apache ActiveMQ is an open source message broker written in Java
    together with a full Java Message Service (JMS) client. The Replicated
    LevelDB Store uses Apache ZooKeeper to pick a master from a set of broker
    nodes configured to replicate a LevelDB Store. Then synchronizes all slave
    LevelDB Stores with the master keeps them up to date by replicating all
    updates from the master. ActiveMQ will preserve the order of messages sent
    by a single producer to all consumers on a topic. If there is a single
    consumer on a queue then the order of messages sent by a single producer
    will be preserved as well. Journal: To achieve high performance of durable
    messaging in ACtiveMQ V4.x we strongly recommend you use our high
    performance journal - which is enabled by default. This works rather like a
    database; messages (and transcation commits/rollbacks and message
    acknowledgements) are written to the journal as fast as is humanly possible
    - then at intervals we checkpoint the journal to the long term persistence
    storage (in this case JDBC).Kind of. A message can be loaded directly from the journal if it was swapped out of memory.
    The journal cannot be used, however, to recover a durable subscription as it
    does not keep an ordered index of messages per durable sub. So when a durable
    sub is activated, the journal checkpoints to flush any messages in the journal
    to the long term store and then the long term store is used to recover the
    durable subscription.Brokers cannot share a journal. Each must be
configured with it's own journal. Broker Clustering: The most common mental model of clustering in
a JMS context is that there is a collection of JMS brokers and a JMS client
will connect to one of them; then if the JMS broker goes down, it will
f we just run multiple brokers on a network and tell the clients about them
using either static discovery or dynamic discovery, then clients can easily
failover from one broker to another. However, stand alone brokers don't know
about consumers on other brokers; so if there are no consumers on a certain
broker, messages could just pile up without being consumedauto-reconnect to
another broker. } 

\item [Rabbit MQ] \hfill \\ {} \item [Zero
        MQ] \hfill \\
    {}
\end{description}


%\subsection{Streaming Broker}
\begin{description}
    \item [Apache Kafka] \hfill \\
        { (What it is) (Creator) (License) (Characteristics according features) Fault-tolerance: Beim conumse kann keine message verloren gehen da log durable, nur bei procuder kann es sein }
    \item [Amazon Kinesis] \hfill \\
    { Amazon Kinesis is a service for real-time processing of streaming big
    data. You can push data from many data producers, rapidly and continuously
as it is generated into Amazon Kinesis, which offers a reliable, highly
scalable service to capture, and store the data. ping that connects all their
distributed systems—DynamoDB, RedShift, S3, etc.—as well as the basis for
distributed stream processing using EC2. \\
 Kinesis keeps messages just for 24 hours no Log Compaction. Thus it cannot be
 used for checkpointing and state store changelogging. Another service must be
 used for durable storage.\\
    
 }
    \item [Scribe] \hfill \\
    {}
    \item [Kastrell] \hfill \\
    {}
    \item [Apache Flume] \hfill \\
    {Flume is a distributed logging service specializing on being a reliable way of getting stream and log data into HDFS.}
\end{description}

\section{Conslusion}

\todo[inline]{Welches System passt für welchen Anwendungsfall und welche nicht (Gründe)}
