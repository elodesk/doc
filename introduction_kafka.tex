\chapter{Apache Kafka}
\section{Background}

- Zitat: First, it is interesting to note that Kafka started out as a way to
make data ingest to Hadoop easier. When there are multiple data sources and
destinations involved, writing a separate data pipeline for each source and
destination pairing quickly evolves to an unmaintainable mess.
\section{Characteristics}
- Zitat: One key feature of Kafka is its functional simplicity. While there is a
lot of sophisticated engineering under the covers, Kafkaâ€™s general functionality
is relatively straightforward. Part of this simplicity comes from its
independence from any other applications (excepting Apache ZooKeeper)

\section{Use Cases}
\begin{description}
    \item [Traditional message broker]
    \item [Log aggregation]
    \item [Stream processing] Kafka's strong durability is very useful in the
        context of stream processing
\end{description}
\section{Technology}

\subsection{Persistance}
- disk are fast with linear writes -> example: 7200rpm raid5 ~600MB/s
- operating system read/write ahead
- keep page cache staying warm and store compact byte structure rather than individual objects
- maintaining coherency between the cache and filesystem in the OS
- All data is immediately written to a persistent log on the filesystem without necessarily flushing to disk. In effect this just means that it is transferred into the kernel's pagecache.see also: https://www.varnish-cache.org/trac/wiki/ArchitectNotes
- Instead of  a per-consumer queue with an associated BTree or other general-purpose random access data structures
  a persistent queue could be built on simple reads and appends to files
  This structure has the advantage that all operations are O(1) and reads do not block writes or each other.
- performance is completely decoupled from the data size
- unlimited disk space without any performance penalty means that we can provide some features like retain messages for a relative long period (say a week)

- for each topic a folder
- separate folders over partitions (1;2) -> topic\_0; topic\_1
- file name: offset of the first message it contains (next file: S bytes (max log size, given in configuration) from previous file)
- file format: sequence of log entries
- log entry: N (4 byte integer -> message length) followed by message (N message bytes)
- message: unique identifier (64 bit integer offset -> byte position of the start of this message in the stream of all messages ever sent to that topic on that partition)

On-disk format of a message:

offset: 8 bytes
message length : 4 bytes (value: 1+4+n) 
magic value  : 1 byte
crc            : 4 bytes
payload        : n bytes
    ?:              : 1 byte
    ?:              : 4 bytes
    message length  : 4 bytes
    message         : n bytes

\subsection{Message Delivery}

\subsection{Replication}

\subsection{Compression}

\subsection{Batching}

\subsection{The Producer}

\subsection{The Consumer}
While many brokered message queue systems have the broker maintain the state of
its consumers, Kafka does not.

\subsubsection{Grouping}

\subsubsection{Ordering (offset)}

\section{Configuration Management (Zookeeper)}
\subsection{PAXO Algorithm}
