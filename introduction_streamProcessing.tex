\chapter{Introduction to Event Streaming}

The view of data as rows of databases or single files changes when one thinks
about what a business actually does with the generated data. Where retail
generates orders they lead to sales, shipments and so on, a financial institution will
generate orders they are going to have an impact of a stock price. Or a social
network platform generates clicks, impressions and searches they are used to
make some sort of intelligent analysis to further display personalized data to
it's users. Such kind of data can be thought of as streams of
events. In fact, collecting all events a business ever generated will lead to the current state
of the business and thus describe what the business did in past. For example the
current price of a stock was generated by all the orders ever made on this
stock. Every order can be captured as an event and so can all events together reproduce
the current stock price.

\section{What is an Event?}
\label{intro-datastream-datastream}
Very basically an event occurs when "something happens"  in a system
like when a user of an online shop adds an item to its basket. In modern systems, events are transmitted as discrete messages on a MOM (see \ref{intro-messaging-mom}) and thus
following Tannenbaum et al. (2006), represent a data unit of a data streams. 
Where a data stream can be applied to discrete as well as continuous media, events are
transmitted as discrete messages only. The message as itself can
be considered as an event message \cite{EIP03}.
\todo[inline]{in the following we'll call messages of a data stream just events}

\section{Processing of Events}
A data stream consisting of events as it self is not valuable but
can be taken advantage of by a system that processes these events and produces a
result. This can be the calculation of the new stock price after a customer sold
his stock or the personalized content on a news feed after a person subscribed to a new fan
page. But it could also be a more complex analysis over all the collected
event that ever happened, stored in a big database. 
\\ \\
In fact, the above mentioned examples differ in it's nature. Where the
calculation of the stock price is fairly simple by setting the price to the
latest paid stock price without any knowledge about the stock prices in past. 
In contrast, a complete analysis over a huge user base will not only require a
significant amount of processing time, it also requires some data produced in the
past. This leads to two different approaches to handle an incoming event stream
of any size: 

\begin{description}
    \item[(A) Store raw data]  \hfill \\
    {Simply storing every single event in a big data store. Through appending
    every incoming event one get a wide history of every activity on the system.
    To analyze the data, a periodic batch process can execute big queries over
    all events to get a result.  $ \Rightarrow $  \textbf{Batch Processing}}
    \item[(B) Store aggregated data  ] \hfill \\
    {Instead of persist every single event, directly process the incoming data stream and store
    only an aggregated summary. Because of updating the aggregation with every
    incoming event, getting an overall result is very fast (what we call
    "Real-Time"). Of course there is not a history of all the "happenings"
    anymore. $ \Rightarrow $ \textbf{Stream Processing}} 
\end{description}
\cite{TalkKleppmann}

\subsection{Batch Processing}
\label{intro-datastream-batchprocessing}
Traditional batch processing systems nowadays are distinguished between
map-reduce based and non map-reduce based systems and typically consists of two
stages, data integration and data analytics. \todo[inline]{ref to literature or glossary} 

The process of data extraction-transformation-load (ETL  \todo{glossary}), 
faced in the data integration
stage, runs at a regular time interval, such as daily, weekly or monthly. 
\\ \\
Analyzing data that resides in a data store is being faced in the
data analytics stage and becomes challenging when data size grows and systems
may not be able to process results within a time limit.
\cite{Liu:2014:SRP:2628194.2628251}

\subsection{Real-time Batch Processing}

As the trend shows, the needs of performance and responsiveness in a big data environment can't be fulfilled with 
traditional batch processing anymore. Instead, real-time processing becomes more 
important than ever to achieve results from queries in minutes, even seconds. 
\cite{bange2013big}

In real-time processing fashion, systems will address the data integration stage
with continual input of data. Processing in near-real-time [glossar] to present 
results within seconds is being addressed in the data analytics stage. Thus,
real-time processing gives organization the ability to take immediate action
for those times when acting within seconds or minutes is significant.
\cite{PrpSvyOfDSPS}

\subsubsection{Lambda Architecture}
The lambda architecture introduces a new paradigm for big data which allows
processing of massive data volumes in near real-time fashion and thus results within
seconds can be achieved. 
\\
Any query is answered through the serving layer by querying 
both the speed and the batch layer. Where the badge layer computes views on the current collected data and
is being outdated at the end of it's computation, the speed layer closes this 
gap by constantly processing the most recent data in near real-time fashion. 

\todo[inline]{Besser einordnen, verweis auf stream processing}

\cite{marz2015big} \cite{PrpSvyOfDSPS}

\subsection{Stream Processing}
\label{intro-datastream-streamprocessing}
Stream processing refers to integration and processing of data before storing. 
A stream processing system is built out of multiple units called a processing
element (PE). Each PE receive input from their input queues, does some
computation on the input using its local state and produce output to their
output queues. PE communicate always through messaging with other PEs. 
\\ \\
Most important, those systems are optimized for high latency and high
availability. Recovering from failures is critical for a stream processing
systems and should be fast and efficient. 
Data should partitioned and handled in parallel for large volumes of data. 
The partitioning strategy of a system  affects how the system
handles the data in parallel and how the system can scale. 
\cite{PrpSvyOfDSPS}
\\ \\
Stream processing frameworks---such as Storm, Samza, or Spark
Streaming---were especially developed to provide rich processing primitives and thus can be taken advantage of
in the data integration- and processing stages.

\subsection{Complex Event processing (CEP)}
In literatur there is often a confusion about the difference between
complex event processing and stream event processing. Both systems work on
events and produce results based on the properties of the events... 
- Zitat: Combines data from multiple sources  to detect patterns and attempt to
identify either opportunities or threats. The goal is to identify significant
events and respond fast. Sales leads, orders or customer service calls are
examples.\\

\todo[inline]{wie Event Sourcing, kurz beschreiben damit man weiss wo man den
Begriff einordnen kann}

\subsection{Event Sourcing}
\label{event-sourcing}
Regarding to event processing we often met the term \textit{Event Sourcing} in
literature. It is a pattern originally defined by Martin Fowler which basically
treats with the same objectives as we mentioned for batch processing just with
other terms of the Domain-Driven-Design community. Instead of directly updating
a database record due to an event, every change of a state should individual
recorded. This leads to a simplified writing sequence (just appending) and gets
also an event history which can be analyzed in detail. 

\section{Stream Broker}
Any system which is dependent on a continuous input of data requires a delivery
system that can provide data constantly. Stream processing
systems, whether being in a lambda architecture or not, obviously holds this requirement
as well as it's especially dependent on low-latency.

But if we think more traditionally, even database systems can be thought of as
an event stream. The process of creating a backup in form of dumps won't scale as
we increase the frequency of dumps over time. Not only will the process take
longer according to the size of the database, also system resources are limited
during this process. An approach to make this more efficient is change
capture, which describes the difference between the state of the affected database
rows before and after the change (basic principle of Event Sourcing
\ref{event-sourcing}). If this can be done continuously a continuous
sequence of row changes is what is being left. This in in fact, can again
be described as a stream of events resulting in a
data stream (\ref{intro-datastream-datastream}).

On the other hand, in a big data environment there is also the requirement of
batch processing systems (\ref{intro-datastream-batchprocessing}) being served
with data. In a lambda environment this could be done again using a stream
processing framework (\ref{intro-datastream-streamprocessing}) responsible for
serving the batch processing system with integrated data, ready for data
analysis. However an other way of doing so would be a data store---such as the
hadoop file system (HDFS)---where data can be directly take for further analysis.
\todo[inline]{hdfs glossary}

The same requirement of a data store holds for any other business intelligence
system followed by the problem of the dependency of a data store which
eventually ---due to the lack of an adapter--- can not be served with data by a
stream processing system as comfortable as the HDFS.

Facing the above described types of processing events ---stream- and batch
processing--- together, data integration results as a common stage both system have to
deal with. Even further, for every system an organization operates, the data
integration stage will be present. 
Additionally, stream processing systems require a continuous
incoming stream to further integrate and process where batch processing systems
on the other hand demand a given persistent set of data to further integrate and analyze.

\begin{table}[h]
\begin{tabular}{l|c|c|cl}
\multicolumn{1}{c|}{\textbf{}} & Data Integration & Continuous Data & Persistent
Data & \multicolumn{1}{c}{} \\ \cline{1-4}
\textbf{Stream Processing}     & x                & x               &
&                      \\
\textbf{Batch Processoing}     & x                &                 & x
&
\end{tabular}
\end{table}

Further more, in terms of stream processing the requirement of low latency is
essential for any system of this type. For database systems combined with stream
processing, reliability becomes significantly important to handle critical updates 
such as replicating the change log as discussed above.

In terms of batch processing however, the demand on low latency is not as
important as the availability of well integrated data with a high throughput to
be able to handle a large volume of data in time range as low as possible.

\begin{table}[h]
\begin{tabular}{l|c|cl}
\multicolumn{1}{c|}{\textbf{}} & \textbf{Stream Processing} & \textbf{Batch
Processing} & \multicolumn{1}{c}{\textbf{}} \\ \cline{1-3}
Data Integration               & x                          & x
&                               \\
Continuous Data                & x                          &
&                               \\
Persistent Data                &                            & x
&                               \\
Low Latency                    & x                          &
&                               \\
Ordering                       &                            & x
&
\end{tabular}
\end{table}

Note that the mentioned systems do not only consume data for integration and
processing, they can also have an output. Thus, many systems are both sources and
destinations for data transfer in consequence of which a system would then need
two channels \todo[inline]{channel->glossary} per system. 
Obviously each output in this constellation is intended to be consumed by 1--N
system(s) again. Connecting all of these would lead to building a custom
channel between each pair of system (see Figure). As the set of system in an organization
grows, this would clearly become a challenge to maintain.

(Figure of messy system landscape)

In this scenario several hurdles arise. The process of data integration would
have to be done for each system pair individually. At this point an obvious
approach for a simplification would be to introduce an organization-wide
standardization of the data format. Thus, the integration process becomes 
significantly easier but still has to be done redundantly. Another problem that
remains are the tightly coupled systems. A simple change on one system could
affect one or more of it's connected systems directly, which is not only hard to
manage but also reduces the flexibility of further development of the landscape.
To extend the landscape the chances are high to touch existing systems which is
not only time intensive but also risky regarding possible failures.

A more elegant and reliable approach to solve the mentioned problems would be to
introduce a broker (\ref{intro-messaging-broker}).

\todo[inline]{refer to problems of point-to-point messaging}
\section{Notes}
- problem of traditional messaging systems


-From traditional PCs and Smartphones to a lot of sensors who are connected to
the interne -> Internet of Things!

\todo[inline]{Einbringen des folgenden Statements: The nice thing about this
architecture is that you can now have multiple consumers for the same event
data. You can have one consumer which simply archives the raw events to some big
storage; even if you don’t yet have the capability to process the raw events,
you might as well store them, since storage is cheap and you can use them in
future. Then you can have another consumer which does some aggregation (for
example, incrementing counters), and another consumer which does something else.
Those can all feed off the same event stream.}


\todo[inline]{move to glossary?}

